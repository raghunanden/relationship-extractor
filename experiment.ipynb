{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation extraction using distant supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpus import read_examples, Corpus, show_examples_for_pair\n",
    "from knowledge_base import read_kb_triples, KB, count_relation_combinations\n",
    "from evaluation import *\n",
    "from dataset import find_unrelated_pairs, split_corpus_and_kb\n",
    "from collections import Counter, defaultdict, namedtuple\n",
    "from EDA import find_common_middles\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "from featurizer import simple_bag_of_words_featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading examples from rel_ext_data/corpus.tsv.gz\n",
      "Read 414123 examples\n"
     ]
    }
   ],
   "source": [
    "examples = read_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to all Spanish-occupied lands . The horno has a beehive shape and uses wood as the only heat source . The procedure still used in parts of New Mexico and Arizona is to build a fire inside the Horno and , when the proper amount of time has passed , remove the embers and ashes and insert the'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = examples[1]\n",
    "' '.join((ex.left, ex.mention_1, ex.middle, ex.mention_2, ex.right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus contains 107820 entities\n",
      "The most common entities are:\n",
      "      9399 India\n",
      "      6214 England\n",
      "      4585 Germany\n",
      "      4486 France\n",
      "      4128 Australia\n",
      "      3939 China\n",
      "      3930 Canada\n",
      "      3897 Italy\n",
      "      3368 California\n",
      "      3125 Pakistan\n",
      "      3103 Europe\n",
      "      3097 New_York_City\n",
      "      3025 London\n",
      "      2470 Japan\n",
      "      2468 United_Kingdom\n",
      "      2279 New_Zealand\n",
      "      2275 New_York\n",
      "      2259 Spain\n",
      "      2132 Philippines\n",
      "      2120 Asia\n"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "for example in examples:\n",
    "    counter[example.entity_1] += 1\n",
    "    counter[example.entity_2] += 1\n",
    "print('The corpus contains {} entities'.format(len(counter)))\n",
    "counts = sorted([(count, key) for key, count in counter.items()], reverse=True)\n",
    "print('The most common entities are:')\n",
    "for count, key in counts[:20]:\n",
    "    print('{:10d} {}'.format(count, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Corpus with 414123 examples"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = Corpus(examples)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first of 9 examples for Steve_Jobs and Pixar is:\n",
      "Example(entity_1='Steve_Jobs', entity_2='Pixar', left='of visual effects on films like The Abyss ( 1989 ) , Terminator 2 ( 1991 ) and Jurassic Park ( 1993 ) The computer graphics division of ILM was bought by', mention_1='Steve Jobs', middle='and became', mention_2='Pixar', right=', who would go on to make several groundbreaking animated films starting with Toy Story ( 1995 ) – more information on the history of that here', left_POS='of/IN visual/JJ effects/NNS on/IN films/NNS like/IN The/DT Abyss/NN -LRB-/-LRB- 1989/CD -RRB-/-RRB- ,/, Terminator/NNP 2/CD -LRB-/-LRB- 1991/CD -RRB-/-RRB- and/CC Jurassic/JJ Park/NN -LRB-/-LRB- 1993/CD -RRB-/-RRB- The/DT computer/NN graphics/NNS division/NN of/IN ILM/NNP was/VBD bought/VBN by/IN', mention_1_POS='Steve/NNP Jobs/NNP', middle_POS='and/CC became/VBD', mention_2_POS='Pixar/NNP', right_POS=',/, who/WP would/MD go/VB on/IN to/TO make/VB several/JJ groundbreaking/VBG animated/JJ films/NNS starting/VBG with/IN Toy/NNP Story/NNP -LRB-/-LRB- 1995/CD -RRB-/-RRB- --/: more/JJR information/NN on/IN the/DT history/NN of/IN that/DT here/RB')\n"
     ]
    }
   ],
   "source": [
    "show_examples_for_pair('Steve_Jobs', 'Pixar', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first of 2 examples for Pixar and Steve_Jobs is:\n",
      "Example(entity_1='Pixar', entity_2='Steve_Jobs', left='in the visual accompaniment to his recordings of Bach ’ s Six Suites for Unaccompanied Cello . Ma has also been seen with Apple Inc. and former', mention_1='Pixar', middle='CEO', mention_2='Steve Jobs', right='. Ma is often invited to press events for Jobs ’ s companies , and has performed on stage during event keynote presentations , as well as appearing in', left_POS=\"in/IN the/DT visual/JJ accompaniment/NN to/TO his/PRP$ recordings/NNS of/IN Bach/NNP '/POS s/NNS Six/CD Suites/NNP for/IN Unaccompanied/NNP Cello/NNP ./. Ma/NNP has/VBZ also/RB been/VBN seen/VBN with/IN Apple/NNP Inc./NNP and/CC former/JJ\", mention_1_POS='Pixar/NNP', middle_POS='CEO/NNP', mention_2_POS='Steve/NNP Jobs/NNP', right_POS=\"./. Ma/NNP is/VBZ often/RB invited/VBN to/TO press/VB events/NNS for/IN Jobs/NNP '/POS s/NNS companies/NNS ,/, and/CC has/VBZ performed/VBN on/IN stage/NN during/IN event/NN keynote/NN presentations/NNS ,/, as/RB well/RB as/IN appearing/VBG in/IN\")\n"
     ]
    }
   ],
   "source": [
    "show_examples_for_pair('Pixar', 'Steve_Jobs', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading KB triples from rel_ext_data/kb.tsv.gz ...\n",
      "Read 56575 KB triples\n"
     ]
    }
   ],
   "source": [
    "kb_triples = read_kb_triples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KB with 56575 triples"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb = KB(kb_triples)\n",
    "kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "all_relations = kb.get_all_relations()\n",
    "print(len(all_relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        2140 adjoins\n",
      "        3316 author\n",
      "         637 capital\n",
      "       22489 contains\n",
      "        4958 film_performance\n",
      "        2404 founders\n",
      "        1012 genre\n",
      "        3280 has_sibling\n",
      "        3774 has_spouse\n",
      "        3153 is_a\n",
      "        1981 nationality\n",
      "        2013 parents\n",
      "        1388 place_of_birth\n",
      "        1031 place_of_death\n",
      "        1526 profession\n",
      "        1473 worked_at\n"
     ]
    }
   ],
   "source": [
    "for rel in all_relations:\n",
    "    print('{:12d} {}'.format(len(kb.get_triples_for_relation(rel)), rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('adjoins', 'Siegburg', 'Bonn')\n",
      "('author', 'Uncle_Silas', 'Sheridan_Le_Fanu')\n",
      "('capital', 'Tunisia', 'Tunis')\n",
      "('contains', 'Brickfields', 'Kuala_Lumpur_Sentral_railway_station')\n",
      "('film_performance', 'Colin_Hanks', 'The_Great_Buck_Howard')\n",
      "('founders', 'Bomis', 'Jimmy_Wales')\n",
      "('genre', 'SPARQL', 'Semantic_Web')\n",
      "('has_sibling', 'Ari_Emanuel', 'Rahm_Emanuel')\n",
      "('has_spouse', 'Percy_Bysshe_Shelley', 'Mary_Shelley')\n",
      "('is_a', 'Bhanu_Athaiya', 'Costume_designer')\n",
      "('nationality', 'Ruben_Rausing', 'Sweden')\n",
      "('parents', 'Prince_Arthur_of_Connaught', 'Prince_Arthur,_Duke_of_Connaught_and_Strathearn')\n",
      "('place_of_birth', 'William_Penny_Brookes', 'Much_Wenlock')\n",
      "('place_of_death', 'Jean_Drapeau', 'Montreal')\n",
      "('profession', 'Rufus_Wainwright', 'Actor')\n",
      "('worked_at', 'Ray_Jackendoff', 'Tufts_University')\n"
     ]
    }
   ],
   "source": [
    "for rel in all_relations:\n",
    "    print(tuple(kb.get_triples_for_relation(rel)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KBTriple(rel='adjoins', sbj='France', obj='Germany')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb.get_triples_for_entities('France', 'Germany')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KBTriple(rel='adjoins', sbj='Germany', obj='France')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb.get_triples_for_entities('Germany', 'France')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KBTriple(rel='founders', sbj='Pixar', obj='Steve_Jobs')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb.get_triples_for_entities('Pixar', 'Steve_Jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KBTriple(rel='worked_at', sbj='Steve_Jobs', obj='Pixar')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb.get_triples_for_entities('Steve_Jobs', 'Pixar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KBTriple(rel='has_sibling', sbj='Cleopatra', obj='Ptolemy_XIII_Theos_Philopator'),\n",
       " KBTriple(rel='has_spouse', sbj='Cleopatra', obj='Ptolemy_XIII_Theos_Philopator')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb.get_triples_for_entities('Cleopatra', 'Ptolemy_XIII_Theos_Philopator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The KB contains 46275 entities\n",
      "The most common entities are:\n",
      "       962 England\n",
      "       815 India\n",
      "       465 London\n",
      "       456 Italy\n",
      "       437 France\n",
      "       420 Germany\n",
      "       412 California\n",
      "       396 United_Kingdom\n",
      "       378 Canada\n",
      "       324 New_York_City\n",
      "       262 Actor\n",
      "       248 New_York\n",
      "       244 Australia\n",
      "       235 China\n",
      "       226 Philippines\n",
      "       224 Japan\n",
      "       223 Russia\n",
      "       214 Scotland\n",
      "       204 Europe\n",
      "       177 Pakistan\n"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "for kbt in kb.get_triples():\n",
    "    counter[kbt.sbj] += 1\n",
    "    counter[kbt.obj] += 1\n",
    "print('The KB contains {} entities'.format(len(counter)))\n",
    "counts = sorted([(count, key) for key, count in counter.items()], reverse=True)\n",
    "print('The most common entities are:')\n",
    "for count, key in counts[:20]:\n",
    "    print('{:10d} {}'.format(count, key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining the corpus and the KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             examples\n",
      "relation               examples    triples    /triple\n",
      "--------               --------    -------    -------\n",
      "adjoins                   85660       2140      40.03\n",
      "author                    15822       3316       4.77\n",
      "capital                   12520        637      19.65\n",
      "contains                  99572      22489       4.43\n",
      "film_performance          11195       4958       2.26\n",
      "founders                   8061       2404       3.35\n",
      "genre                      1941       1012       1.92\n",
      "has_sibling               12332       3280       3.76\n",
      "has_spouse                16188       3774       4.29\n",
      "is_a                       6955       3153       2.21\n",
      "nationality                4649       1981       2.35\n",
      "parents                    5387       2013       2.68\n",
      "place_of_birth             2214       1388       1.60\n",
      "place_of_death             2047       1031       1.99\n",
      "profession                 2876       1526       1.88\n",
      "worked_at                  4494       1473       3.05\n"
     ]
    }
   ],
   "source": [
    "def count_examples(corpus, kb):\n",
    "    counter = Counter()\n",
    "    for rel in all_relations:\n",
    "        for kbt in kb.get_triples_for_relation(rel):\n",
    "            # count examples in both forward and reverse directions\n",
    "            counter[rel] += len(corpus.get_examples_for_entities(kbt.sbj, kbt.obj))\n",
    "            counter[rel] += len(corpus.get_examples_for_entities(kbt.obj, kbt.sbj))\n",
    "    # report results\n",
    "    print('{:20s} {:>10s} {:>10s} {:>10s}'.format('', '', '', 'examples'))\n",
    "    print('{:20s} {:>10s} {:>10s} {:>10s}'.format('relation', 'examples', 'triples', '/triple'))\n",
    "    print('{:20s} {:>10s} {:>10s} {:>10s}'.format('--------', '--------', '-------', '-------'))\n",
    "    for rel in all_relations:\n",
    "        nx = counter[rel]\n",
    "        nt = len(kb.get_triples_for_relation(rel))\n",
    "        print('{:20s} {:10d} {:10d} {:10.2f}'.format(rel, nx, nt, 1.0 * nx / nt))\n",
    "        \n",
    "count_examples(corpus, kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 301073 unrelated pairs, including:\n",
      "    ('South_Africa', 'Afghanistan')\n",
      "    ('Detroit_Zoo', 'Detroit_Institute_of_Arts')\n",
      "    ('Charles_Darwin', 'Ernst_Mayr')\n",
      "    ('Buddy_Rich', 'Ben_Webster')\n",
      "    ('The_Flip_Wilson_Show', 'Some_Like_It_Hot')\n",
      "    ('Edmund_Burke', 'Jeremy_Bentham')\n",
      "    ('Ty_Cobb', 'Miller_Huggins')\n",
      "    ('Fred_Lynn', 'Jim_Rice')\n",
      "    ('Metallica', 'Guns_and_Roses')\n",
      "    ('Cantilan', 'Lanuza')\n"
     ]
    }
   ],
   "source": [
    "unrelated_pairs = find_unrelated_pairs(corpus, kb)\n",
    "print('Found {} unrelated pairs, including:'.format(len(unrelated_pairs)))\n",
    "for pair in list(unrelated_pairs)[:10]:\n",
    "    print('   ', pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common relation combinations are:\n",
      "      1526 ('is_a', 'profession')\n",
      "       495 ('capital', 'contains')\n",
      "       183 ('place_of_birth', 'place_of_death')\n",
      "        76 ('nationality', 'place_of_birth')\n",
      "        11 ('nationality', 'place_of_death')\n",
      "        11 ('adjoins', 'contains')\n",
      "         8 ('has_sibling', 'has_spouse')\n",
      "         3 ('nationality', 'place_of_birth', 'place_of_death')\n",
      "         2 ('parents', 'worked_at')\n",
      "         1 ('nationality', 'worked_at')\n",
      "         1 ('has_spouse', 'parents')\n",
      "         1 ('author', 'founders')\n"
     ]
    }
   ],
   "source": [
    "count_relation_combinations(kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_datasets(corpus, kb, include_positive=True, sampling_rate=0.1, seed=1, KBTriple = namedtuple('KBTriple', 'rel, sbj, obj')):\n",
    "    unrelated_pairs = find_unrelated_pairs(corpus, kb)\n",
    "    random.seed(seed)\n",
    "    unrelated_pairs = random.sample(unrelated_pairs, int(sampling_rate * len(unrelated_pairs)))\n",
    "    kbts_by_rel = defaultdict(list)\n",
    "    labels_by_rel = defaultdict(list)\n",
    "    for index, rel in enumerate(all_relations):\n",
    "        if include_positive:\n",
    "            for kbt in kb.get_triples_for_relation(rel):\n",
    "                kbts_by_rel[rel].append(kbt)\n",
    "                labels_by_rel[rel].append(True)\n",
    "        for sbj, obj in unrelated_pairs:\n",
    "            kbts_by_rel[rel].append(KBTriple(rel, sbj, obj))\n",
    "            labels_by_rel[rel].append(False)        \n",
    "    return kbts_by_rel, labels_by_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbts_by_rel, labels_by_rel = build_datasets(corpus, kb, include_positive=True, sampling_rate=0.1, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = split_corpus_and_kb(corpus, kb,seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_datasets_for_split(split, include_positive=True, sampling_rate=0.1, seed=1):\n",
    "    return build_datasets(data[split]['corpus'], data[split]['kb'], include_positive, sampling_rate, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(classifier, test_split='dev', verbose=True):\n",
    "    test_kbts_by_rel, true_labels_by_rel = build_datasets_for_split(test_split)\n",
    "    results = {}\n",
    "    if verbose:\n",
    "        print_statistics_header()\n",
    "    for rel in all_relations:\n",
    "        pred_labels = classifier(test_kbts_by_rel[rel])\n",
    "        stats = precision_recall_fscore_support(true_labels_by_rel[rel], pred_labels, beta=0.5)\n",
    "        stats = [stat[1] for stat in stats]  # stats[1] is the stat for label True\n",
    "        stats.append(len(pred_labels))  # number of examples\n",
    "        results[rel] = stats\n",
    "        if verbose:\n",
    "            print_statistics_row(rel, results[rel])\n",
    "    avg_result = macro_average_results(results)\n",
    "    if verbose:\n",
    "        print_statistics_footer(avg_result)\n",
    "    return avg_result[2]  # return f_0.5 score as summary statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lift(f):\n",
    "    return lambda xs: [f(x) for x in xs]\n",
    "\n",
    "def make_random_classifier(p=0.50):\n",
    "    def random_classify(kb_triple):\n",
    "        return random.random() < p\n",
    "    return lift(random_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.058      0.515      0.070        303       5319\n",
      "author                    0.088      0.508      0.106        480       5496\n",
      "capital                   0.019      0.539      0.024         89       5105\n",
      "contains                  0.349      0.502      0.371       2667       7683\n",
      "film_performance          0.138      0.491      0.162        822       5838\n",
      "founders                  0.064      0.482      0.078        359       5375\n",
      "genre                     0.039      0.608      0.048        166       5182\n",
      "has_sibling               0.092      0.493      0.109        513       5529\n",
      "has_spouse                0.110      0.530      0.130        575       5591\n",
      "is_a                      0.099      0.547      0.119        494       5510\n",
      "nationality               0.054      0.463      0.066        311       5327\n",
      "parents                   0.062      0.502      0.075        325       5341\n",
      "place_of_birth            0.040      0.488      0.049        217       5233\n",
      "place_of_death            0.028      0.490      0.034        145       5161\n",
      "profession                0.052      0.563      0.063        245       5261\n",
      "worked_at                 0.047      0.544      0.058        228       5244\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.084      0.517      0.098       7939      88195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09757501010273492"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(make_random_classifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjoins              fwd         8461 ,\n",
      "adjoins              fwd         5633 and\n",
      "adjoins              fwd          993 , and\n",
      "adjoins              fwd         5599 ,\n",
      "adjoins              fwd         3780 and\n",
      "adjoins              fwd          680 , and\n",
      "author               fwd         1214 by\n",
      "author               fwd          155 ,\n",
      "author               fwd          130 , by\n",
      "author               fwd         1106 's\n",
      "author               fwd          294 ‘ s\n",
      "author               fwd          175 ’ s\n",
      "capital              fwd           37 ,\n",
      "capital              fwd           19 in\n",
      "capital              fwd           18 (\n",
      "capital              fwd         3711 ,\n",
      "capital              fwd          178 in\n",
      "capital              fwd           87 , the capital of\n",
      "contains             fwd          460 's\n",
      "contains             fwd          355 ,\n",
      "contains             fwd          250 (\n",
      "contains             fwd        25095 ,\n",
      "contains             fwd         5603 in\n",
      "contains             fwd          668 in the\n",
      "film_performance     fwd          286 in\n",
      "film_performance     fwd          200 's\n",
      "film_performance     fwd          115 film\n",
      "film_performance     fwd          213 with\n",
      "film_performance     fwd          152 , starring\n",
      "film_performance     fwd          115 opposite\n",
      "founders             fwd           98 founder\n",
      "founders             fwd           59 co-founder\n",
      "founders             fwd           57 ,\n",
      "founders             fwd          180 's\n",
      "founders             fwd          104 of\n",
      "founders             fwd           77 ‘ s\n",
      "genre                fwd           28 , a\n",
      "genre                fwd           13 in 1994 , he became a central figure in the\n",
      "genre                fwd           11 is a\n",
      "genre                fwd          122 ,\n",
      "genre                fwd           62 series\n",
      "genre                fwd           23 \n",
      "has_sibling          fwd         1369 and\n",
      "has_sibling          fwd          614 ,\n",
      "has_sibling          fwd          139 , and\n",
      "has_sibling          fwd          930 and\n",
      "has_sibling          fwd          460 ,\n",
      "has_sibling          fwd           94 , and\n",
      "has_spouse           fwd         2029 and\n",
      "has_spouse           fwd          375 ,\n",
      "has_spouse           fwd          112 and his wife\n",
      "has_spouse           fwd         1382 and\n",
      "has_spouse           fwd          271 ,\n",
      "has_spouse           fwd           78 and his wife\n",
      "is_a                 fwd          120 ,\n",
      "is_a                 fwd           77 and\n",
      "is_a                 fwd           34 , a\n",
      "is_a                 fwd          252 ,\n",
      "is_a                 fwd          175 and\n",
      "is_a                 fwd           81 \n",
      "nationality          fwd          331 of\n",
      "nationality          fwd           79 in\n",
      "nationality          fwd           34 of the\n",
      "nationality          fwd           57 ,\n",
      "nationality          fwd           27 by\n",
      "nationality          fwd           25 under\n",
      "parents              fwd           77 , son of\n",
      "parents              fwd           50 and\n",
      "parents              fwd           44 ,\n",
      "parents              fwd          177 and\n",
      "parents              fwd          167 ,\n",
      "parents              fwd           47 and his son\n",
      "place_of_birth       fwd           90 of\n",
      "place_of_birth       fwd           64 was born in\n",
      "place_of_birth       fwd           37 in\n",
      "place_of_birth       fwd           17 ,\n",
      "place_of_birth       fwd           16 by\n",
      "place_of_birth       fwd           11 under\n",
      "place_of_death       fwd           73 in\n",
      "place_of_death       fwd           63 of\n",
      "place_of_death       fwd           17 at\n",
      "place_of_death       fwd           12 ,\n",
      "place_of_death       fwd           10 under\n",
      "place_of_death       fwd            9 mayor\n",
      "profession           fwd           65 ,\n",
      "profession           fwd           27 , a\n",
      "profession           fwd           20 and\n",
      "profession           fwd          114 ,\n",
      "profession           fwd           74 \n",
      "profession           fwd           24 and\n",
      "worked_at            fwd          103 of\n",
      "worked_at            fwd           82 at\n",
      "worked_at            fwd           66 's\n",
      "worked_at            fwd           37 ,\n",
      "worked_at            fwd           30 founder\n",
      "worked_at            fwd           25 co-founder\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fwd': defaultdict(<function EDA.find_common_middles.<locals>.<lambda>>,\n",
       "             {'adjoins': {',', ', and', 'and'},\n",
       "              'author': {',', ', by', 'by'},\n",
       "              'capital': {'(', ',', 'in'},\n",
       "              'contains': {\"'s\", '(', ','},\n",
       "              'film_performance': {\"'s\", 'film', 'in'},\n",
       "              'founders': {',', 'co-founder', 'founder'},\n",
       "              'genre': {', a',\n",
       "               'in 1994 , he became a central figure in the',\n",
       "               'is a'},\n",
       "              'has_sibling': {',', ', and', 'and'},\n",
       "              'has_spouse': {',', 'and', 'and his wife'},\n",
       "              'is_a': {',', ', a', 'and'},\n",
       "              'nationality': {'in', 'of', 'of the'},\n",
       "              'parents': {',', ', son of', 'and'},\n",
       "              'place_of_birth': {'in', 'of', 'was born in'},\n",
       "              'place_of_death': {'at', 'in', 'of'},\n",
       "              'profession': {',', ', a', 'and'},\n",
       "              'worked_at': {\"'s\", 'at', 'of'}}),\n",
       " 'rev': defaultdict(<function EDA.find_common_middles.<locals>.<lambda>>,\n",
       "             {'adjoins': {',', ', and', 'and'},\n",
       "              'author': {\"'s\", '‘ s', '’ s'},\n",
       "              'capital': {',', ', the capital of', 'in'},\n",
       "              'contains': {',', 'in', 'in the'},\n",
       "              'film_performance': {', starring', 'opposite', 'with'},\n",
       "              'founders': {\"'s\", 'of', '‘ s'},\n",
       "              'genre': {'', ',', 'series'},\n",
       "              'has_sibling': {',', ', and', 'and'},\n",
       "              'has_spouse': {',', 'and', 'and his wife'},\n",
       "              'is_a': {'', ',', 'and'},\n",
       "              'nationality': {',', 'by', 'under'},\n",
       "              'parents': {',', 'and', 'and his son'},\n",
       "              'place_of_birth': {',', 'by', 'under'},\n",
       "              'place_of_death': {',', 'mayor', 'under'},\n",
       "              'profession': {'', ',', 'and'},\n",
       "              'worked_at': {',', 'co-founder', 'founder'}})}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_common_middles(data,all_relations,show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_top_k_middles_classifier(train_split='train', top_k=3):\n",
    "    corpus = data[train_split]['corpus']\n",
    "    top_k_mids_by_rel = find_common_middles(data,all_relations,split=train_split, top_k=top_k)\n",
    "    def classify(kb_triple):\n",
    "        fwd_mids = top_k_mids_by_rel['fwd'][kb_triple.rel]\n",
    "        rev_mids = top_k_mids_by_rel['rev'][kb_triple.rel]\n",
    "        for ex in corpus.get_examples_for_entities(kb_triple.sbj, kb_triple.obj):\n",
    "            if ex.middle in fwd_mids:\n",
    "                return True\n",
    "        for ex in corpus.get_examples_for_entities(kb_triple.obj, kb_triple.sbj):\n",
    "            if ex.middle in rev_mids:\n",
    "                return True\n",
    "        return False\n",
    "    return lift(classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.337      0.406      0.349        303       5319\n",
      "author                    0.228      0.058      0.144        480       5496\n",
      "capital                   0.101      0.191      0.111         89       5105\n",
      "contains                  0.537      0.066      0.220       2667       7683\n",
      "film_performance          0.400      0.002      0.012        822       5838\n",
      "founders                  0.196      0.061      0.136        359       5375\n",
      "genre                     0.000      0.000      0.000        166       5182\n",
      "has_sibling               0.320      0.222      0.294        513       5529\n",
      "has_spouse                0.380      0.249      0.344        575       5591\n",
      "is_a                      0.021      0.010      0.017        494       5510\n",
      "nationality               0.133      0.039      0.089        311       5327\n",
      "parents                   0.086      0.068      0.082        325       5341\n",
      "place_of_birth            0.060      0.023      0.046        217       5233\n",
      "place_of_death            0.013      0.007      0.011        145       5161\n",
      "profession                0.009      0.008      0.008        245       5261\n",
      "worked_at                 0.083      0.031      0.062        228       5244\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.182      0.090      0.120       7939      88195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1203641255018017"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(train_top_k_middles_classifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_datasets(\n",
    "        kbts_by_rel,\n",
    "        corpus,\n",
    "        featurizers=[simple_bag_of_words_featurizer],\n",
    "        vectorizer=None):\n",
    "    # Create feature counters for all instances (kbts).\n",
    "    feat_counters_by_rel = defaultdict(list)\n",
    "    for rel, kbts in kbts_by_rel.items():\n",
    "        for kbt in kbts:\n",
    "            feature_counter = Counter()\n",
    "            for featurizer in featurizers:\n",
    "                featurizer(kbt, corpus, feature_counter)\n",
    "            feat_counters_by_rel[rel].append(feature_counter)\n",
    "    feat_matrices_by_rel = defaultdict(list)\n",
    "    # If we haven't been given a Vectorizer, create one and fit it to all the feature counters.\n",
    "    if vectorizer == None:\n",
    "        vectorizer = DictVectorizer(sparse=True)\n",
    "        def traverse_dicts():\n",
    "            for dict_list in feat_counters_by_rel.values():\n",
    "                for d in dict_list:\n",
    "                    yield d\n",
    "        vectorizer.fit(traverse_dicts())\n",
    "    # Now use the Vectorizer to transform feature dictionaries into feature matrices.\n",
    "    for rel, feat_counters in feat_counters_by_rel.items():\n",
    "        feat_matrices_by_rel[rel] = vectorizer.transform(feat_counters)\n",
    "    return feat_matrices_by_rel, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_models(\n",
    "        split='train',\n",
    "        featurizers=[simple_bag_of_words_featurizer],\n",
    "        model_factory=lambda: LogisticRegression(fit_intercept=True),\n",
    "        verbose=True):\n",
    "    if verbose: print('Building datasets')\n",
    "    train_o, train_y = build_datasets_for_split(split=split)\n",
    "    if verbose: print('Featurizing')\n",
    "    train_X, vectorizer = featurize_datasets(train_o, data[split]['corpus'], featurizers)\n",
    "    models = {}\n",
    "    if verbose: print('Training models')\n",
    "    for rel in all_relations:\n",
    "        models[rel] = model_factory()\n",
    "        models[rel].fit(train_X[rel], train_y[rel])\n",
    "    if verbose: print('Training complete\\n')\n",
    "    return {\n",
    "        'featurizers': featurizers,\n",
    "        'vectorizer': vectorizer,\n",
    "        'models': models,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(split, featurizers, vectorizer, models):\n",
    "    test_o, test_y = build_datasets_for_split(split=split)\n",
    "    test_X, _ = featurize_datasets(test_o, data[split]['corpus'], featurizers, vectorizer=vectorizer)\n",
    "    predictions = {}\n",
    "    for rel in all_relations:\n",
    "        predictions[rel] = models[rel].predict(test_X[rel])\n",
    "    return predictions, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_predictions(predictions, test_y, verbose=True):\n",
    "    results = {}  # one result row for each relation\n",
    "    if verbose:\n",
    "        print_statistics_header()\n",
    "    for rel in all_relations:\n",
    "        stats = precision_recall_fscore_support(test_y[rel], predictions[rel], beta=0.5)\n",
    "        stats = [stat[1] for stat in stats]  # stats[1] is the stat for label True\n",
    "        stats.append(len(test_y[rel]))\n",
    "        results[rel] = stats\n",
    "        if verbose:\n",
    "            print_statistics_row(rel, results[rel])\n",
    "    avg_result = macro_average_results(results)\n",
    "    if verbose:\n",
    "        print_statistics_footer(avg_result)\n",
    "    return avg_result[2]  # return f_0.5 score as summary statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def experiment(\n",
    "        train_split='train',\n",
    "        test_split='dev',\n",
    "        featurizers=[simple_bag_of_words_featurizer],\n",
    "        model_factory=lambda: LogisticRegression(fit_intercept=True),\n",
    "        verbose=True):\n",
    "    train_result = train_models(train_split, featurizers, model_factory, verbose)\n",
    "    predictions, test_y = predict(test_split,\n",
    "                                  featurizers,\n",
    "                                  train_result['vectorizer'],\n",
    "                                  train_result['models'])\n",
    "    evaluate_predictions(predictions, test_y, verbose)\n",
    "    return train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building datasets\n",
      "Featurizing\n",
      "Training models\n",
      "Training complete\n",
      "\n",
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.944      0.449      0.774        303       5319\n",
      "author                    0.812      0.548      0.740        480       5496\n",
      "capital                   0.704      0.213      0.482         89       5105\n",
      "contains                  0.755      0.606      0.720       2667       7683\n",
      "film_performance          0.807      0.569      0.745        822       5838\n",
      "founders                  0.800      0.401      0.667        359       5375\n",
      "genre                     0.623      0.229      0.463        166       5182\n",
      "has_sibling               0.861      0.242      0.569        513       5529\n",
      "has_spouse                0.912      0.360      0.698        575       5591\n",
      "is_a                      0.690      0.235      0.497        494       5510\n",
      "nationality               0.531      0.138      0.339        311       5327\n",
      "parents                   0.868      0.585      0.791        325       5341\n",
      "place_of_birth            0.776      0.207      0.501        217       5233\n",
      "place_of_death            0.559      0.131      0.338        145       5161\n",
      "profession                0.662      0.176      0.426        245       5261\n",
      "worked_at                 0.667      0.254      0.503        228       5244\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.748      0.334      0.578       7939      88195\n"
     ]
    }
   ],
   "source": [
    "_ = experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def examine_model_weights(\n",
    "        train_split='train',\n",
    "        featurizers=[simple_bag_of_words_featurizer],\n",
    "        model_factory=lambda: LogisticRegression(fit_intercept=True),\n",
    "        k=3,\n",
    "        verbose=True):\n",
    "    train_result = train_models(train_split, featurizers, model_factory, verbose)\n",
    "    feature_names = train_result['vectorizer'].get_feature_names()\n",
    "    for rel, model in train_result['models'].items():\n",
    "        print('Highest and lowest feature weights for relation {}:\\n'.format(rel))\n",
    "        sorted_weights = sorted([(wgt, idx) for idx, wgt in enumerate(model.coef_[0])], reverse=True)\n",
    "        for wgt, idx in sorted_weights[:k]:\n",
    "            print('{:10.3f} {}'.format(wgt, feature_names[idx]))\n",
    "        print('{:>10s} {}'.format('.....', '.....'))\n",
    "        for wgt, idx in sorted_weights[-k:]:\n",
    "            print('{:10.3f} {}'.format(wgt, feature_names[idx]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building datasets\n",
      "Featurizing\n",
      "Training models\n",
      "Training complete\n",
      "\n",
      "Highest and lowest feature weights for relation adjoins:\n",
      "\n",
      "     2.557 Córdoba\n",
      "     2.399 Taluks\n",
      "     1.771 Southwest\n",
      "     ..... .....\n",
      "    -1.301 22\n",
      "    -1.324 India\n",
      "    -1.499 Cook\n",
      "\n",
      "Highest and lowest feature weights for relation author:\n",
      "\n",
      "     2.728 author\n",
      "     2.512 Szathmáry\n",
      "     2.409 book\n",
      "     ..... .....\n",
      "    -2.133 or\n",
      "    -2.378 directed\n",
      "    -2.999 1818\n",
      "\n",
      "Highest and lowest feature weights for relation capital:\n",
      "\n",
      "     3.625 capital\n",
      "     1.887 headquarters\n",
      "     1.765 km\n",
      "     ..... .....\n",
      "    -1.470 largest\n",
      "    -1.666 Madras\n",
      "    -1.951 Antrim\n",
      "\n",
      "Highest and lowest feature weights for relation contains:\n",
      "\n",
      "     3.652 Channel\n",
      "     2.235 continent\n",
      "     2.117 districts\n",
      "     ..... .....\n",
      "    -2.375 Cook\n",
      "    -2.394 appeared\n",
      "    -4.158 Antrim\n",
      "\n",
      "Highest and lowest feature weights for relation film_performance:\n",
      "\n",
      "     4.081 starring\n",
      "     3.584 alongside\n",
      "     3.355 opposite\n",
      "     ..... .....\n",
      "    -1.734 or\n",
      "    -1.922 then\n",
      "    -3.863 double\n",
      "\n",
      "Highest and lowest feature weights for relation founders:\n",
      "\n",
      "     3.983 founded\n",
      "     3.838 founder\n",
      "     3.442 co-founder\n",
      "     ..... .....\n",
      "    -1.869 band\n",
      "    -1.891 writing\n",
      "    -2.006 top\n",
      "\n",
      "Highest and lowest feature weights for relation genre:\n",
      "\n",
      "     3.210 series\n",
      "     2.873 \n",
      "     2.725 album\n",
      "     ..... .....\n",
      "    -1.440 and\n",
      "    -1.459 '\n",
      "    -1.987 at\n",
      "\n",
      "Highest and lowest feature weights for relation has_sibling:\n",
      "\n",
      "     5.216 brother\n",
      "     4.374 sister\n",
      "     3.006 Marlon\n",
      "     ..... .....\n",
      "    -1.557 grandson\n",
      "    -1.691 He\n",
      "    -1.951 formed\n",
      "\n",
      "Highest and lowest feature weights for relation has_spouse:\n",
      "\n",
      "     5.321 wife\n",
      "     4.113 married\n",
      "     4.050 widow\n",
      "     ..... .....\n",
      "    -1.667 In\n",
      "    -1.782 He\n",
      "    -1.986 unfaithful\n",
      "\n",
      "Highest and lowest feature weights for relation is_a:\n",
      "\n",
      "     3.923 stage\n",
      "     3.359 \n",
      "     2.900 family\n",
      "     ..... .....\n",
      "    -2.073 Pavo\n",
      "    -2.434 compares\n",
      "    -3.341 Latidae\n",
      "\n",
      "Highest and lowest feature weights for relation nationality:\n",
      "\n",
      "     2.370 born\n",
      "     2.091 -born\n",
      "     1.909 King\n",
      "     ..... .....\n",
      "    -1.375 American\n",
      "    -1.530 or\n",
      "    -2.073 state\n",
      "\n",
      "Highest and lowest feature weights for relation parents:\n",
      "\n",
      "     5.065 son\n",
      "     4.986 daughter\n",
      "     4.356 father\n",
      "     ..... .....\n",
      "    -1.686 Gamal\n",
      "    -1.722 filmmaker\n",
      "    -1.973 sadistic\n",
      "\n",
      "Highest and lowest feature weights for relation place_of_birth:\n",
      "\n",
      "     3.717 born\n",
      "     3.075 mayor\n",
      "     2.931 birthplace\n",
      "     ..... .....\n",
      "    -1.381 and\n",
      "    -1.426 or\n",
      "    -1.838 state\n",
      "\n",
      "Highest and lowest feature weights for relation place_of_death:\n",
      "\n",
      "     2.440 assassinated\n",
      "     2.266 executed\n",
      "     2.148 died\n",
      "     ..... .....\n",
      "    -1.093 or\n",
      "    -1.189 and\n",
      "    -1.822 state\n",
      "\n",
      "Highest and lowest feature weights for relation profession:\n",
      "\n",
      "     3.648 \n",
      "     2.760 American\n",
      "     2.277 philosopher\n",
      "     ..... .....\n",
      "    -1.310 between\n",
      "    -1.334 in\n",
      "    -1.440 at\n",
      "\n",
      "Highest and lowest feature weights for relation worked_at:\n",
      "\n",
      "     3.172 CEO\n",
      "     2.986 president\n",
      "     2.939 professor\n",
      "     ..... .....\n",
      "    -1.386 novel\n",
      "    -1.642 or\n",
      "    -1.703 state\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examine_model_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_new_relation_instances(\n",
    "        train_split='train',\n",
    "        test_split='dev',\n",
    "        featurizers=[simple_bag_of_words_featurizer],\n",
    "        model_factory=lambda: LogisticRegression(fit_intercept=True),\n",
    "        k=10,\n",
    "        verbose=True):\n",
    "\n",
    "    # train models\n",
    "    train_result = train_models(train_split, featurizers, model_factory, verbose)\n",
    "\n",
    "    # build datasets for negative instances only\n",
    "    neg_o, neg_y = build_datasets_for_split(test_split, include_positive=False, sampling_rate=1.0)\n",
    "    neg_X, _ = featurize_datasets(neg_o,\n",
    "                                  data[test_split]['corpus'],\n",
    "                                  featurizers,\n",
    "                                  train_result['vectorizer'])\n",
    "\n",
    "    # report highest confidence predictions\n",
    "    for rel, model in train_result['models'].items():\n",
    "        print('Highest probability examples for relation {}:\\n'.format(rel))\n",
    "        probs = model.predict_proba(neg_X[rel])\n",
    "        probs = [prob[1] for prob in probs] # probability for class True\n",
    "        sorted_probs = sorted([(p, idx) for idx, p in enumerate(probs)], reverse=True)\n",
    "        for p, idx in sorted_probs[:k]:\n",
    "            print('{:10.3f} {}'.format(p, neg_o[rel][idx]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building datasets\n",
      "Featurizing\n",
      "Training models\n",
      "Training complete\n",
      "\n",
      "Highest probability examples for relation adjoins:\n",
      "\n",
      "     1.000 KBTriple(rel='adjoins', sbj='Sun', obj='Moon')\n",
      "     1.000 KBTriple(rel='adjoins', sbj='Moon', obj='Sun')\n",
      "     1.000 KBTriple(rel='adjoins', sbj='India', obj='Maharashtra')\n",
      "     1.000 KBTriple(rel='adjoins', sbj='Maharashtra', obj='India')\n",
      "     1.000 KBTriple(rel='adjoins', sbj='Kashmir', obj='India')\n",
      "     1.000 KBTriple(rel='adjoins', sbj='India', obj='Kashmir')\n",
      "     1.000 KBTriple(rel='adjoins', sbj='Europe', obj='Great_Britain')\n",
      "     1.000 KBTriple(rel='adjoins', sbj='Great_Britain', obj='Europe')\n",
      "     0.999 KBTriple(rel='adjoins', sbj='Ukraine', obj='Chernobyl_Nuclear_Power_Plant')\n",
      "     0.999 KBTriple(rel='adjoins', sbj='Chernobyl_Nuclear_Power_Plant', obj='Ukraine')\n",
      "\n",
      "Highest probability examples for relation author:\n",
      "\n",
      "     1.000 KBTriple(rel='author', sbj='The_Doors_of_Perception', obj='Aldous_Huxley')\n",
      "     1.000 KBTriple(rel='author', sbj='Aldous_Huxley', obj='The_Doors_of_Perception')\n",
      "     1.000 KBTriple(rel='author', sbj='Systema_Naturae', obj='Carl_Linnaeus')\n",
      "     1.000 KBTriple(rel='author', sbj='Carl_Linnaeus', obj='Systema_Naturae')\n",
      "     1.000 KBTriple(rel='author', sbj='Charlie_and_the_Chocolate_Factory', obj='Roald_Dahl')\n",
      "     1.000 KBTriple(rel='author', sbj='Roald_Dahl', obj='Charlie_and_the_Chocolate_Factory')\n",
      "     1.000 KBTriple(rel='author', sbj='Carl_Jung', obj='Psychological_Types')\n",
      "     1.000 KBTriple(rel='author', sbj='Psychological_Types', obj='Carl_Jung')\n",
      "     1.000 KBTriple(rel='author', sbj='Neil_Gaiman', obj='American_Gods')\n",
      "     1.000 KBTriple(rel='author', sbj='American_Gods', obj='Neil_Gaiman')\n",
      "\n",
      "Highest probability examples for relation capital:\n",
      "\n",
      "     1.000 KBTriple(rel='capital', sbj='Ukraine', obj='Chernobyl_Nuclear_Power_Plant')\n",
      "     1.000 KBTriple(rel='capital', sbj='Chernobyl_Nuclear_Power_Plant', obj='Ukraine')\n",
      "     1.000 KBTriple(rel='capital', sbj='Italy', obj='Rome')\n",
      "     1.000 KBTriple(rel='capital', sbj='Rome', obj='Italy')\n",
      "     1.000 KBTriple(rel='capital', sbj='Republic_of_Ireland', obj='Blarney')\n",
      "     1.000 KBTriple(rel='capital', sbj='Blarney', obj='Republic_of_Ireland')\n",
      "     1.000 KBTriple(rel='capital', sbj='Philippines', obj='Ifugao')\n",
      "     1.000 KBTriple(rel='capital', sbj='Ifugao', obj='Philippines')\n",
      "     1.000 KBTriple(rel='capital', sbj='Johor', obj='Johor_Bahru')\n",
      "     1.000 KBTriple(rel='capital', sbj='Johor_Bahru', obj='Johor')\n",
      "\n",
      "Highest probability examples for relation contains:\n",
      "\n",
      "     1.000 KBTriple(rel='contains', sbj='India', obj='Maharashtra')\n",
      "     1.000 KBTriple(rel='contains', sbj='Ukraine', obj='Chernobyl_Nuclear_Power_Plant')\n",
      "     1.000 KBTriple(rel='contains', sbj='India', obj='Uttar_Pradesh')\n",
      "     1.000 KBTriple(rel='contains', sbj='Chernobyl_Nuclear_Power_Plant', obj='Ukraine')\n",
      "     1.000 KBTriple(rel='contains', sbj='Italy', obj='Rome')\n",
      "     1.000 KBTriple(rel='contains', sbj='Uttar_Pradesh', obj='India')\n",
      "     1.000 KBTriple(rel='contains', sbj='Rome', obj='Italy')\n",
      "     1.000 KBTriple(rel='contains', sbj='Maharashtra', obj='India')\n",
      "     1.000 KBTriple(rel='contains', sbj='Roman_Empire', obj='Rome')\n",
      "     1.000 KBTriple(rel='contains', sbj='Rome', obj='Roman_Empire')\n",
      "\n",
      "Highest probability examples for relation film_performance:\n",
      "\n",
      "     1.000 KBTriple(rel='film_performance', sbj='Hong_Kong', obj='Shanghai_Noon')\n",
      "     1.000 KBTriple(rel='film_performance', sbj='Shanghai_Noon', obj='Hong_Kong')\n",
      "     1.000 KBTriple(rel='film_performance', sbj='The_Pink_Panther_2', obj='Harald_Zwart')\n",
      "     1.000 KBTriple(rel='film_performance', sbj='Harald_Zwart', obj='The_Pink_Panther_2')\n",
      "     1.000 KBTriple(rel='film_performance', sbj='Francis_Ford_Coppola', obj='Robin_Williams')\n",
      "     1.000 KBTriple(rel='film_performance', sbj='Robin_Williams', obj='Francis_Ford_Coppola')\n",
      "     1.000 KBTriple(rel='film_performance', sbj='Salman_Khan', obj='Tere_Naam')\n",
      "     1.000 KBTriple(rel='film_performance', sbj='Tere_Naam', obj='Salman_Khan')\n",
      "     1.000 KBTriple(rel='film_performance', sbj='Josephus', obj='Antiquities_of_the_Jews')\n",
      "     1.000 KBTriple(rel='film_performance', sbj='Antiquities_of_the_Jews', obj='Josephus')\n",
      "\n",
      "Highest probability examples for relation founders:\n",
      "\n",
      "     1.000 KBTriple(rel='founders', sbj='Abraham', obj='Ishmael')\n",
      "     1.000 KBTriple(rel='founders', sbj='Ishmael', obj='Abraham')\n",
      "     1.000 KBTriple(rel='founders', sbj='L._Ron_Hubbard', obj='Church_of_Scientology')\n",
      "     1.000 KBTriple(rel='founders', sbj='Church_of_Scientology', obj='L._Ron_Hubbard')\n",
      "     1.000 KBTriple(rel='founders', sbj='Lepidoptera', obj='Insect')\n",
      "     1.000 KBTriple(rel='founders', sbj='Insect', obj='Lepidoptera')\n",
      "     1.000 KBTriple(rel='founders', sbj='Adam_Weishaupt', obj='Illuminati')\n",
      "     1.000 KBTriple(rel='founders', sbj='Illuminati', obj='Adam_Weishaupt')\n",
      "     1.000 KBTriple(rel='founders', sbj='Systema_Naturae', obj='Carl_Linnaeus')\n",
      "     1.000 KBTriple(rel='founders', sbj='Carl_Linnaeus', obj='Systema_Naturae')\n",
      "\n",
      "Highest probability examples for relation genre:\n",
      "\n",
      "     1.000 KBTriple(rel='genre', sbj=\"Dexter's_Laboratory\", obj='Cartoon_Cartoons')\n",
      "     1.000 KBTriple(rel='genre', sbj='Cartoon_Cartoons', obj=\"Dexter's_Laboratory\")\n",
      "     0.999 KBTriple(rel='genre', sbj='Taylor_York', obj='All_We_Know_Is_Falling')\n",
      "     0.999 KBTriple(rel='genre', sbj='All_We_Know_Is_Falling', obj='Taylor_York')\n",
      "     0.999 KBTriple(rel='genre', sbj='Falcon', obj='Lanner_falcon')\n",
      "     0.999 KBTriple(rel='genre', sbj='Lanner_falcon', obj='Falcon')\n",
      "     0.996 KBTriple(rel='genre', sbj='Family_Guy', obj='Meg_Griffin')\n",
      "     0.996 KBTriple(rel='genre', sbj='Meg_Griffin', obj='Family_Guy')\n",
      "     0.995 KBTriple(rel='genre', sbj='Tattoo_artist', obj='Tattoo_artist')\n",
      "     0.986 KBTriple(rel='genre', sbj='Rock_music', obj='Phrazes_for_the_Young')\n",
      "\n",
      "Highest probability examples for relation has_sibling:\n",
      "\n",
      "     1.000 KBTriple(rel='has_sibling', sbj='Abraham', obj='Ishmael')\n",
      "     1.000 KBTriple(rel='has_sibling', sbj='Ishmael', obj='Abraham')\n",
      "     1.000 KBTriple(rel='has_sibling', sbj='Sergey_Brin', obj='Larry_Page')\n",
      "     1.000 KBTriple(rel='has_sibling', sbj='Larry_Page', obj='Sergey_Brin')\n",
      "     1.000 KBTriple(rel='has_sibling', sbj='Isaac', obj='Abraham')\n",
      "     1.000 KBTriple(rel='has_sibling', sbj='Abraham', obj='Isaac')\n",
      "     1.000 KBTriple(rel='has_sibling', sbj='Anne_Boleyn', obj='Elizabeth_I_of_England')\n",
      "     1.000 KBTriple(rel='has_sibling', sbj='Elizabeth_I_of_England', obj='Anne_Boleyn')\n",
      "     1.000 KBTriple(rel='has_sibling', sbj='Janet_Leigh', obj='Jamie_Lee_Curtis')\n",
      "     1.000 KBTriple(rel='has_sibling', sbj='Jamie_Lee_Curtis', obj='Janet_Leigh')\n",
      "\n",
      "Highest probability examples for relation has_spouse:\n",
      "\n",
      "     1.000 KBTriple(rel='has_spouse', sbj='Sergey_Brin', obj='Larry_Page')\n",
      "     1.000 KBTriple(rel='has_spouse', sbj='Larry_Page', obj='Sergey_Brin')\n",
      "     1.000 KBTriple(rel='has_spouse', sbj='Anne_Boleyn', obj='Elizabeth_I_of_England')\n",
      "     1.000 KBTriple(rel='has_spouse', sbj='Elizabeth_I_of_England', obj='Anne_Boleyn')\n",
      "     1.000 KBTriple(rel='has_spouse', sbj='Isidor_Straus', obj='Denver')\n",
      "     1.000 KBTriple(rel='has_spouse', sbj='Denver', obj='Isidor_Straus')\n",
      "     1.000 KBTriple(rel='has_spouse', sbj='Indira_Gandhi', obj='Rajiv_Gandhi')\n",
      "     1.000 KBTriple(rel='has_spouse', sbj='Rajiv_Gandhi', obj='Indira_Gandhi')\n",
      "     1.000 KBTriple(rel='has_spouse', sbj='Karl_Marx', obj='Friedrich_Engels')\n",
      "     1.000 KBTriple(rel='has_spouse', sbj='Friedrich_Engels', obj='Karl_Marx')\n",
      "\n",
      "Highest probability examples for relation is_a:\n",
      "\n",
      "     1.000 KBTriple(rel='is_a', sbj='Apidae', obj='Bee')\n",
      "     1.000 KBTriple(rel='is_a', sbj='Bee', obj='Apidae')\n",
      "     1.000 KBTriple(rel='is_a', sbj='Lepidoptera', obj='Insect')\n",
      "     1.000 KBTriple(rel='is_a', sbj='Insect', obj='Lepidoptera')\n",
      "     1.000 KBTriple(rel='is_a', sbj='Passerine', obj='Corvidae')\n",
      "     1.000 KBTriple(rel='is_a', sbj='Corvidae', obj='Passerine')\n",
      "     1.000 KBTriple(rel='is_a', sbj='Hibiscus', obj='Malvaceae')\n",
      "     1.000 KBTriple(rel='is_a', sbj='Malvaceae', obj='Hibiscus')\n",
      "     1.000 KBTriple(rel='is_a', sbj='Malvaceae', obj='Okra')\n",
      "     1.000 KBTriple(rel='is_a', sbj='Okra', obj='Malvaceae')\n",
      "\n",
      "Highest probability examples for relation nationality:\n",
      "\n",
      "     1.000 KBTriple(rel='nationality', sbj='Systema_Naturae', obj='Carl_Linnaeus')\n",
      "     1.000 KBTriple(rel='nationality', sbj='Carl_Linnaeus', obj='Systema_Naturae')\n",
      "     1.000 KBTriple(rel='nationality', sbj='Matale_District', obj='Sri_Lanka')\n",
      "     1.000 KBTriple(rel='nationality', sbj='Sri_Lanka', obj='Matale_District')\n",
      "     1.000 KBTriple(rel='nationality', sbj='North_Island', obj='New_Zealand')\n",
      "     1.000 KBTriple(rel='nationality', sbj='New_Zealand', obj='North_Island')\n",
      "     1.000 KBTriple(rel='nationality', sbj='Anne_Boleyn', obj='Elizabeth_I_of_England')\n",
      "     1.000 KBTriple(rel='nationality', sbj='Elizabeth_I_of_England', obj='Anne_Boleyn')\n",
      "     1.000 KBTriple(rel='nationality', sbj='California', obj='San_Francisco_Bay_Area')\n",
      "     1.000 KBTriple(rel='nationality', sbj='San_Francisco_Bay_Area', obj='California')\n",
      "\n",
      "Highest probability examples for relation parents:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1.000 KBTriple(rel='parents', sbj='Isaac', obj='Abraham')\n",
      "     1.000 KBTriple(rel='parents', sbj='Abraham', obj='Isaac')\n",
      "     1.000 KBTriple(rel='parents', sbj='Abraham', obj='Ishmael')\n",
      "     1.000 KBTriple(rel='parents', sbj='Ishmael', obj='Abraham')\n",
      "     1.000 KBTriple(rel='parents', sbj='Kim_Jong-il', obj='Kim_Jong-un')\n",
      "     1.000 KBTriple(rel='parents', sbj='Kim_Jong-un', obj='Kim_Jong-il')\n",
      "     1.000 KBTriple(rel='parents', sbj='Anne_Boleyn', obj='Elizabeth_I_of_England')\n",
      "     1.000 KBTriple(rel='parents', sbj='Elizabeth_I_of_England', obj='Anne_Boleyn')\n",
      "     1.000 KBTriple(rel='parents', sbj='Indira_Gandhi', obj='Rajiv_Gandhi')\n",
      "     1.000 KBTriple(rel='parents', sbj='Rajiv_Gandhi', obj='Indira_Gandhi')\n",
      "\n",
      "Highest probability examples for relation place_of_birth:\n",
      "\n",
      "     1.000 KBTriple(rel='place_of_birth', sbj='Matale_District', obj='Sri_Lanka')\n",
      "     1.000 KBTriple(rel='place_of_birth', sbj='Sri_Lanka', obj='Matale_District')\n",
      "     1.000 KBTriple(rel='place_of_birth', sbj='North_Island', obj='New_Zealand')\n",
      "     1.000 KBTriple(rel='place_of_birth', sbj='New_Zealand', obj='North_Island')\n",
      "     1.000 KBTriple(rel='place_of_birth', sbj='Abraham', obj='Ishmael')\n",
      "     1.000 KBTriple(rel='place_of_birth', sbj='Ishmael', obj='Abraham')\n",
      "     1.000 KBTriple(rel='place_of_birth', sbj='Systema_Naturae', obj='Carl_Linnaeus')\n",
      "     1.000 KBTriple(rel='place_of_birth', sbj='Carl_Linnaeus', obj='Systema_Naturae')\n",
      "     1.000 KBTriple(rel='place_of_birth', sbj='Illinois', obj='United_States_Senate')\n",
      "     1.000 KBTriple(rel='place_of_birth', sbj='United_States_Senate', obj='Illinois')\n",
      "\n",
      "Highest probability examples for relation place_of_death:\n",
      "\n",
      "     1.000 KBTriple(rel='place_of_death', sbj='Abraham', obj='Ishmael')\n",
      "     1.000 KBTriple(rel='place_of_death', sbj='Ishmael', obj='Abraham')\n",
      "     1.000 KBTriple(rel='place_of_death', sbj='Systema_Naturae', obj='Carl_Linnaeus')\n",
      "     1.000 KBTriple(rel='place_of_death', sbj='Carl_Linnaeus', obj='Systema_Naturae')\n",
      "     1.000 KBTriple(rel='place_of_death', sbj='Matale_District', obj='Sri_Lanka')\n",
      "     1.000 KBTriple(rel='place_of_death', sbj='Sri_Lanka', obj='Matale_District')\n",
      "     1.000 KBTriple(rel='place_of_death', sbj='North_Island', obj='New_Zealand')\n",
      "     1.000 KBTriple(rel='place_of_death', sbj='New_Zealand', obj='North_Island')\n",
      "     1.000 KBTriple(rel='place_of_death', sbj='Ukraine', obj='Chernobyl_Nuclear_Power_Plant')\n",
      "     1.000 KBTriple(rel='place_of_death', sbj='Chernobyl_Nuclear_Power_Plant', obj='Ukraine')\n",
      "\n",
      "Highest probability examples for relation profession:\n",
      "\n",
      "     0.999 KBTriple(rel='profession', sbj='Hispania', obj='Spain')\n",
      "     0.999 KBTriple(rel='profession', sbj='Spain', obj='Hispania')\n",
      "     0.999 KBTriple(rel='profession', sbj='Eyeless_in_Gaza', obj='Aldous_Huxley')\n",
      "     0.999 KBTriple(rel='profession', sbj='Aldous_Huxley', obj='Eyeless_in_Gaza')\n",
      "     0.998 KBTriple(rel='profession', sbj='Abraham', obj='Ishmael')\n",
      "     0.998 KBTriple(rel='profession', sbj='Ishmael', obj='Abraham')\n",
      "     0.998 KBTriple(rel='profession', sbj='Tilly_Smith', obj='Phuket')\n",
      "     0.998 KBTriple(rel='profession', sbj='Phuket', obj='Tilly_Smith')\n",
      "     0.996 KBTriple(rel='profession', sbj='Screenwriter', obj='Actor')\n",
      "     0.996 KBTriple(rel='profession', sbj='Actor', obj='Screenwriter')\n",
      "\n",
      "Highest probability examples for relation worked_at:\n",
      "\n",
      "     1.000 KBTriple(rel='worked_at', sbj='Matale_District', obj='Sri_Lanka')\n",
      "     1.000 KBTriple(rel='worked_at', sbj='Sri_Lanka', obj='Matale_District')\n",
      "     1.000 KBTriple(rel='worked_at', sbj='North_Island', obj='New_Zealand')\n",
      "     1.000 KBTriple(rel='worked_at', sbj='New_Zealand', obj='North_Island')\n",
      "     1.000 KBTriple(rel='worked_at', sbj='Lepidoptera', obj='Insect')\n",
      "     1.000 KBTriple(rel='worked_at', sbj='Insect', obj='Lepidoptera')\n",
      "     1.000 KBTriple(rel='worked_at', sbj='Austria', obj='Gaston_Glock')\n",
      "     1.000 KBTriple(rel='worked_at', sbj='Gaston_Glock', obj='Austria')\n",
      "     1.000 KBTriple(rel='worked_at', sbj='L._Ron_Hubbard', obj='Church_of_Scientology')\n",
      "     1.000 KBTriple(rel='worked_at', sbj='Church_of_Scientology', obj='L._Ron_Hubbard')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "find_new_relation_instances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
